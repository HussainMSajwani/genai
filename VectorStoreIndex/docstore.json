{"docstore/metadata": {"01feec41-2935-4ad1-ac8f-17354b789117": {"doc_hash": "c18be9112ea55638559ef1e6c87a7e4bf72f934543c0f3ede10cc2eb704d5a09"}, "1ca58f63-b363-4954-a209-68340789753f": {"doc_hash": "a062dd4952b935e430d409c4a5e5cd760e8781e3200d5c7e12a232dddba8a528"}, "d707b104-1e20-4760-9da6-942c039dcbc0": {"doc_hash": "a062dd4952b935e430d409c4a5e5cd760e8781e3200d5c7e12a232dddba8a528"}, "900ace75-20ab-4fdf-8cbc-e3ce8d88e19f": {"doc_hash": "cb1596c1ac177011885f835cfa02178b2036826fca6119d7023bbb7e944d097c"}, "a1116637-ab73-489c-b220-d706da1e3e50": {"doc_hash": "7ad3fedd4515814b48fcaa8740399ca79371559e71f7077d034abe8ee2b5758c"}, "68528a0f-c5e0-4bd0-b3ce-7d493340cf24": {"doc_hash": "3cbff6ca295cfdb88dc06b3d3476d92cc441072b58f0b2de70596e07865525e6"}, "8d5b6ac9-1a1a-4ef1-b3d5-182a5dee8d56": {"doc_hash": "31d6de373a83cbba501170ea60fd7bf911592b7fc617f4fba3092031d2616750"}, "005190b1-7690-4b09-9610-ec80d5def234": {"doc_hash": "c921f503b472e3d5a5ed5cc38aa74416d5b66783fc10b2bbbffacf3e9ff9bcea"}, "10fe652f-188a-4851-b215-45f623ed6968": {"doc_hash": "eaf155000fab4ca01adbb2fd83e627d077e278543f10a6b96941c5cf1282649a"}, "fcc3daa5-9848-4b04-b22b-d70f46998c9b": {"doc_hash": "9c6684861f1c901c4347c09ba4a99d03871800c785fdc370f6a44759d879499e"}, "26cec0a0-7740-4494-a32c-542a64074d9e": {"doc_hash": "486226f1bcf3a89831ee23c3f108ba2ff1e266ed4f5f4142cd1b96663544b597"}, "6c7c29b3-c610-4a9d-b017-da04dc149173": {"doc_hash": "0e8da228781f3502944e42ce4b2dffa867dbc6469c66ed21a52a7e7a220537dd"}, "2a4f5885-091b-4cdb-b8ff-ccbd1e9a00a1": {"doc_hash": "b2d28bdb30eceeb25c376a8c0561e63aa186f2ef3874e736e45c246ffa248fb6"}, "a3e94c23-c886-4da2-a54a-8ec59d5d4425": {"doc_hash": "3d30286570e8cff4c8125c945489091cd646e28940bf8b26e1a0f43c2ad84e57"}, "385fc94f-a48c-4e65-93c5-a73ca8c2426c": {"doc_hash": "30568df8a2bedf95724b73ebe4dcfefc0071736e85a9ae736e8e973f99597719"}, "d0fcab45-e0b9-4042-9171-53ee21293747": {"doc_hash": "2f4baef1545df0662a666963d1c47d23ffcd27a43dd3a42592381fdc0823d704"}, "e39865cb-1fbf-4c65-947c-cbd38fd0141b": {"doc_hash": "8899e1c1ef0ca52329e14930a37078762542b48dc9a2aeac3da29332668e47f3"}, "498af3cf-4320-4885-953d-7ba32db1c6b5": {"doc_hash": "dbdf7ca67d07563eb03c8fe71d5c10b91ced7949fab4d6bf9d5a6d60635e5725"}, "9e143ce2-864f-41be-96f1-5a0bf69de35f": {"doc_hash": "aa76f3c721b6c4233e6ee78a08c81697a8b781f7a4ac62ae1b2727da78d2ae96"}, "339615d0-f5d0-433d-9ac0-306877a93a7b": {"doc_hash": "e684a9f8ebf9dd8d3de4825764c943541d6f031011e34a231f793631e4869f3b"}, "07de0b85-5d5c-4e9e-bb0f-5229ae05e54f": {"doc_hash": "be04bece0bc8b7273a8b3283d66982f8cbbc2b80d7f0574e183d58c54dc7c7f9"}, "2e4959f8-9e24-4bdd-a252-97dcdc6635b0": {"doc_hash": "b36a4ea1d0cce7c57dc2f017f3f468851ee8bc9f49aeb883b1d167c2b116af89"}, "8cb94f33-d107-4f9f-9011-72cea3304c74": {"doc_hash": "600ed71744c04db284eb47f4ffe22de4fc9716167b22d64af25547e133494f97"}, "75a21967-0a6f-4ebe-bf81-3a71f06c60b5": {"doc_hash": "8b1f2179eb7fdeaa247e51f515d57da1b5f482dd6375834bbae377061411f3e6"}, "38f0359a-78c3-45d0-9c7a-197c1bf30cef": {"doc_hash": "69af9bf0e1294bb032ae7278c81fabd17063ffc7076c6864890d158edac494ff"}, "04d7ebc5-25bc-44b9-8989-8b22762e1bcb": {"doc_hash": "63b8dd1a82e12f814750f64bffc6ba1553e36001921421552c3dc7f7245421dc"}, "dc24781d-5656-4316-9a7d-cc588e50d7bc": {"doc_hash": "a469fb977f106cd932bb27bdf6289d7dc51b87b026e7f1040a3832bb2cb7945c"}, "e374ceb0-c889-433f-ba89-d157a81e2013": {"doc_hash": "67b8ef0274bdf2005aa4cb80a973fce35862ce293a75563ef8eb90f1de6a9dd8"}, "96cf3362-3210-436c-b929-54fb78fde30b": {"doc_hash": "e84de6517825acc133ec3db8e697853666e7ae713880a13987cd3c4387f9043a"}, "258e7ffd-9b5f-44c0-822c-e1e494ee93a0": {"doc_hash": "a153dd8569783e1e0822862f77e6e720ec343d487b62ff403a19b0dc831f27d9"}, "fbdeb71b-afda-4635-b4ab-fca1b6552ebf": {"doc_hash": "bdfa3e1240dfdc99c7885f334ebbf2744380933fd669a3b4dae0f41949e07019"}, "81e23f77-902f-41e0-bf0b-c1e86cb25b2c": {"doc_hash": "a93e88be29e0d935d344eccacbf7aa03db5e1af09cffad6a1cb918af08a2a2d7"}, "8dd55eee-e366-4973-901f-d974b47b8314": {"doc_hash": "25a2bef8b5623c9c957711fbadb24ae5963561c2833d3aff8f000e44cdfdd1e8"}, "12ae4899-a053-4ee3-8892-ab9c758e5361": {"doc_hash": "bffc52f8b6af03a4636b6f8795094559c9dd9ac40b9158a545dce96657ab5f2b"}, "c390adb4-8ec2-4730-aef4-45823777b1df": {"doc_hash": "aeaad0a5748b7263ea7850372728ffd4e72de5d75e49552c8d887b4873519b6b"}, "e0d6fc45-5bf0-47b4-8aa8-458b8f9af142": {"doc_hash": "1f926cd31365e888636d3276a86984de5a40371e80b083ebcdc37e0cc19a2bec"}, "be3639b8-96ad-44fb-a5b9-d0e5326a546f": {"doc_hash": "c5355ea671ae09b15243cca627d58da6e0a0d59a221f1a9aedc6f0c226c706b4"}, "4de7f4fa-cf73-41cd-9ecd-96fb3e2a0bb2": {"doc_hash": "4c11d7247321efd855a4ec0a318f41025e5385d01cd32cac3ba6a7f6fd08a999"}, "ecff2333-5b06-4ee3-883f-26b9a93db345": {"doc_hash": "3889ae4d444a6ea07431d76802b8a1c5e72a20e9a771d12c950d28075ba6f5f4"}, "c9255197-4bd9-4166-a132-dfa29d9a3f72": {"doc_hash": "3918c9a87dd47a7cd71c329453627a54a2ec221a5a8d88a61c5e15cb83b638e6"}, "38570b03-279d-4e85-a9bc-6ec240d656e9": {"doc_hash": "669214308c5467f3ddace91db333fde1e141d8d71083afe4340ff1a712b8d0bf", "ref_doc_id": "01feec41-2935-4ad1-ac8f-17354b789117"}, "ec11c14b-8114-4905-9939-2770c915d9f4": {"doc_hash": "de4c2193f94abf1a1e9e211e22ad8de48fdc512d4ac77fc5fefc0ee321738399", "ref_doc_id": "900ace75-20ab-4fdf-8cbc-e3ce8d88e19f"}, "f39eb7b3-9bbe-4dae-a82b-96b08c82a64b": {"doc_hash": "90f5c8a57b6c32d5a5b8ddfe11d307e6a6d2b8b42af354195e90b623b270cc78", "ref_doc_id": "a1116637-ab73-489c-b220-d706da1e3e50"}, "cc87b453-d772-47d1-bc0d-b52c80082ca1": {"doc_hash": "79db2edafe5a31563e853f78a7214bc7edc90f09413ee8bd70a513441db22147", "ref_doc_id": "68528a0f-c5e0-4bd0-b3ce-7d493340cf24"}, "f68946de-52bb-4526-a8d5-5960099baf68": {"doc_hash": "bdd8680ad094c7f8b2b3c947374239d0d390eeb0f792baf28989f28a8b5b307f", "ref_doc_id": "8d5b6ac9-1a1a-4ef1-b3d5-182a5dee8d56"}, "bb0466fa-131e-485c-a833-d2cd3fbcef6c": {"doc_hash": "bdf636dfca64d3b5accdbb765639c7d6e7a6e3eb5957249da49458135d862d97", "ref_doc_id": "005190b1-7690-4b09-9610-ec80d5def234"}, "bce798b2-2a4e-446e-a8ab-46908c6f5c79": {"doc_hash": "81707022c6713e92102336121d48e328f5408f34491aa9038a35ee9bc59a0bf8", "ref_doc_id": "10fe652f-188a-4851-b215-45f623ed6968"}, "c566e29a-3b32-464b-bf97-c02007bd1d41": {"doc_hash": "576752294aa77f4dc9d1f9f2a9158f39df5e4e1dfa305d0c9af35f5a25057fda", "ref_doc_id": "fcc3daa5-9848-4b04-b22b-d70f46998c9b"}, "0a670611-b839-481d-b722-de5d76d071f3": {"doc_hash": "fcae2367908cedd941af543409bb63b345903ee25dbb17be9825a254a3b98ef1", "ref_doc_id": "26cec0a0-7740-4494-a32c-542a64074d9e"}, "63d76fa9-05f7-4929-9b27-9c0e5320d73f": {"doc_hash": "0308e4689e3137222e63563ace15b96a2c9fbd33baaa98cc1f96519bb47ddadb", "ref_doc_id": "6c7c29b3-c610-4a9d-b017-da04dc149173"}, "e0e97d01-0a5b-4f0b-b3ad-5aa51014ae6e": {"doc_hash": "bbd70e4bdce1baf2be48738810746583d421e92031e190c88d7e20bbc298b51c", "ref_doc_id": "2a4f5885-091b-4cdb-b8ff-ccbd1e9a00a1"}, "530b337d-6090-4f8d-a172-e21eddf7f338": {"doc_hash": "ddd45264b243a38db9dedd91664b92c99e06512f46106c9799274beba3edbcde", "ref_doc_id": "a3e94c23-c886-4da2-a54a-8ec59d5d4425"}, "58f7540f-87d2-46da-b8b7-d4199a3199f7": {"doc_hash": "30b9df1cf8f2cb32ae487c7b331dd8df4f60d1bb93baa636223ed5e354ae6de7", "ref_doc_id": "385fc94f-a48c-4e65-93c5-a73ca8c2426c"}, "7ffe4dad-4ee7-43f9-8a3f-d2277e88524f": {"doc_hash": "b1a1457af80d91fb93036b70909fcd08a5d99e19c4e7fe634c02b8dd8d156064", "ref_doc_id": "385fc94f-a48c-4e65-93c5-a73ca8c2426c"}, "758a5dc7-4a9f-4ac4-bc01-246e743231b3": {"doc_hash": "7f4bd9409fd5990aa0c42b8e2bcff9a18e0c108aaff9cb5275abf23c81e1bc32", "ref_doc_id": "d0fcab45-e0b9-4042-9171-53ee21293747"}, "01031466-5c9c-423f-b7f3-fd8daea93440": {"doc_hash": "3f37da2242a9e2aa3086b183f017a9181bc26323f98b8fa834bef3c6bfbd7bd2", "ref_doc_id": "e39865cb-1fbf-4c65-947c-cbd38fd0141b"}, "e2fcdd68-a88e-493c-8bf9-80f4819c3871": {"doc_hash": "27c230472de1184ea14d9b919f4ba26c1909fd536e0331100e16dbcaa4b94969", "ref_doc_id": "498af3cf-4320-4885-953d-7ba32db1c6b5"}, "4e440a22-d429-400a-9629-61894ca41a74": {"doc_hash": "73ef5523d868e0de003fc53d9bf5c225b11a04c824f90aff60ff275e2492a3d9", "ref_doc_id": "9e143ce2-864f-41be-96f1-5a0bf69de35f"}, "991e4599-295d-4c87-a7dc-95bf18328d07": {"doc_hash": "9fc67e4ccce7777e3d8575c493a38b652d4a8a0acb4e9f4c934fb60f8abff8db", "ref_doc_id": "339615d0-f5d0-433d-9ac0-306877a93a7b"}, "0e625d39-4fbd-484e-ba9d-d570fec90736": {"doc_hash": "b87bb3f0b08f96e978ee6471e892bb9baf2b1123540ddafd5f2992e78070b66f", "ref_doc_id": "07de0b85-5d5c-4e9e-bb0f-5229ae05e54f"}, "f380dc30-8c8c-47ee-9dc5-6a9905a6686b": {"doc_hash": "c66135580a0c97dd8b6f33ea15d4430d2492964624485352b33e4f1873148dee", "ref_doc_id": "2e4959f8-9e24-4bdd-a252-97dcdc6635b0"}, "96d682e8-cb07-487c-a02f-37529fbf6dd7": {"doc_hash": "9b0a57a21fa61974675b77890437b420e46862653047aa877e6de19a98355a07", "ref_doc_id": "8cb94f33-d107-4f9f-9011-72cea3304c74"}, "97237e60-5ade-426b-9f8b-05836002b1b3": {"doc_hash": "e18cfe34dd5422cdabb358a63507dfeb979443f3208bdb25af524c2d67257678", "ref_doc_id": "8cb94f33-d107-4f9f-9011-72cea3304c74"}, "db2966a7-8354-40d1-9686-4132769d315c": {"doc_hash": "b07d00c9a50b2a8b469c4349670ca13ea100eaae9a6c863080cd97f2bfc8a5bb", "ref_doc_id": "75a21967-0a6f-4ebe-bf81-3a71f06c60b5"}, "0a179080-2643-4c5f-b8fa-0f5fb4a3fac6": {"doc_hash": "9e3e3d5fc4581f8d1c4ac515b4db84db13c0beafb034a79860867fc42b2f1442", "ref_doc_id": "38f0359a-78c3-45d0-9c7a-197c1bf30cef"}, "18b5855a-da3d-4584-824d-e19bcd3bba44": {"doc_hash": "b7f6e76a9f53cce566c69773c5f00d01763dfb1e8cfd678c6fe729191b8ba656", "ref_doc_id": "04d7ebc5-25bc-44b9-8989-8b22762e1bcb"}, "a3374b56-7286-43c9-98f7-519553572b5b": {"doc_hash": "926afdc7d559c93555ab997074f4e817ce5813e3b7c5a79d0c84d23f13b0be41", "ref_doc_id": "dc24781d-5656-4316-9a7d-cc588e50d7bc"}, "df9f576c-0cd5-41b2-a0d8-fab0e1950c79": {"doc_hash": "89f633f3a886e13852f91ccc1ecdda503321866cdc0eba3b4ee8c47c4d2b3ec2", "ref_doc_id": "e374ceb0-c889-433f-ba89-d157a81e2013"}, "47fdb869-eeb8-463b-ae30-688374ae0b26": {"doc_hash": "5ef517a532efb8c9f07ea324e35ed23bff62d552f64a055be77d2a8d6e15b597", "ref_doc_id": "96cf3362-3210-436c-b929-54fb78fde30b"}, "1ddf20be-db42-4c26-9b29-78da837c1715": {"doc_hash": "c4c0d6d82676589a4a446df1260b189e955bff53c016e88fcf747665921b2837", "ref_doc_id": "258e7ffd-9b5f-44c0-822c-e1e494ee93a0"}, "ccf64be1-25a9-4da9-b6d7-f694a3a3e988": {"doc_hash": "bfac2612f011c92ed5135830f430f9082eaf85b38855d0b4599b28c0c0613a7a", "ref_doc_id": "fbdeb71b-afda-4635-b4ab-fca1b6552ebf"}, "bdebe034-3c1c-48a8-a506-330c5fe931aa": {"doc_hash": "54d9b6f6fdc1b64f4167d181ad66663e1bcf89f08566447b682c2af9f4437641", "ref_doc_id": "81e23f77-902f-41e0-bf0b-c1e86cb25b2c"}, "4d7297a0-a687-4234-af3a-820f144a0af3": {"doc_hash": "032f4f61dd1a10490e94348b96692bd2920b7aa7e79aad294e360b0b50523751", "ref_doc_id": "8dd55eee-e366-4973-901f-d974b47b8314"}, "0a76e208-2432-4f0c-8391-01a8c9afb239": {"doc_hash": "76b09cb0b1532b75bd746f62c35f062ed586df31e15ba7273aa302fe99859b89", "ref_doc_id": "12ae4899-a053-4ee3-8892-ab9c758e5361"}, "97c7d7f6-5331-4672-9313-526ed8a7f5e6": {"doc_hash": "ef0bcb7d71677c35db53155b6d5891e4cadc8c3d914297055c974999c2d8ac83", "ref_doc_id": "12ae4899-a053-4ee3-8892-ab9c758e5361"}, "c99391ae-b035-410b-bc27-d8f44176a2ac": {"doc_hash": "42d192ce83d8d04eeef094eeefadc6e71016d4c11fee40f5e209d0bb38bdda7f", "ref_doc_id": "c390adb4-8ec2-4730-aef4-45823777b1df"}, "df1823a8-f3da-4d8a-be8b-d1ccd0a5aef1": {"doc_hash": "5eb41f11183fe4f7f12513138f413857d860b3f4325be7aa3a5a671aabcb9cc6", "ref_doc_id": "c390adb4-8ec2-4730-aef4-45823777b1df"}, "e1c69f8d-2abd-4eb3-a321-af818bf56394": {"doc_hash": "b6fb638bac02824d4d7184741344e3448973859ddc0e08045bb62548f48b09b2", "ref_doc_id": "e0d6fc45-5bf0-47b4-8aa8-458b8f9af142"}, "6e788d27-0441-4418-8390-87bbf0ee3e47": {"doc_hash": "4922c31477aea559f67d89a6ef84d3b2dffbd9e6402a0cb7ac86a2463c2814a2", "ref_doc_id": "be3639b8-96ad-44fb-a5b9-d0e5326a546f"}, "5dc1a04d-91f1-4904-96db-bbc4cc567810": {"doc_hash": "18df7c862c4d4109f255aaea49c2c58b6aada3eb725862c1628a3786900d5a3c", "ref_doc_id": "4de7f4fa-cf73-41cd-9ecd-96fb3e2a0bb2"}, "754ce08f-2ebb-4ea8-aa43-9be63b847d82": {"doc_hash": "f27c989309f81386a3bdf0da422eb2a49ca5ad5808b191ef521b20efb31b6836", "ref_doc_id": "ecff2333-5b06-4ee3-883f-26b9a93db345"}, "0d0e65c3-7a80-44b8-b0c2-edb997716aad": {"doc_hash": "96895fb3e306ace025f552e4be488297399bbed4210ced86723fe5ce40f2acaf", "ref_doc_id": "c9255197-4bd9-4166-a132-dfa29d9a3f72"}, "6b024771-a7d8-487f-b6b4-97a86fde813d": {"doc_hash": "10fe891e4549b08ff83afc596ba9dfd5a99558d48682d86463ea0b4a22b157d0", "ref_doc_id": "c9255197-4bd9-4166-a132-dfa29d9a3f72"}}, "docstore/data": {"38570b03-279d-4e85-a9bc-6ec240d656e9": {"__data__": {"id_": "38570b03-279d-4e85-a9bc-6ec240d656e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01feec41-2935-4ad1-ac8f-17354b789117", "node_type": null, "metadata": {}, "hash": "c18be9112ea55638559ef1e6c87a7e4bf72f934543c0f3ede10cc2eb704d5a09"}}, "hash": "669214308c5467f3ddace91db333fde1e141d8d71083afe4340ff1a712b8d0bf", "text": "#event-based\n#frame-based\n#algorithm\n#material\n#simulation\n#experimental\n#application\n#neuro-vbts \n#3d-reconstruction\nList of research group:\naric \nbristol\nmcube\nethz (Sferrazza)", "start_char_idx": 0, "end_char_idx": 178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec11c14b-8114-4905-9939-2770c915d9f4": {"__data__": {"id_": "ec11c14b-8114-4905-9939-2770c915d9f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "900ace75-20ab-4fdf-8cbc-e3ce8d88e19f", "node_type": null, "metadata": {}, "hash": "cb1596c1ac177011885f835cfa02178b2036826fca6119d7023bbb7e944d097c"}}, "hash": "de4c2193f94abf1a1e9e211e22ad8de48fdc512d4ac77fc5fefc0ee321738399", "text": "#event-based\n#frame-based\n#algorithm\n#material\n#simulation\n#experimental\n#application\n#neuro-vbts \n#3d-reconstruction\n#RL\n#grasping\n#sim2real\n#pickAndPlace\n\nList of research group:\naric \nbristol\nmcube\nethz (Sferrazza)\nStanford (Li Fei-Fei & Jeannette Bohg)", "start_char_idx": 0, "end_char_idx": 256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f39eb7b3-9bbe-4dae-a82b-96b08c82a64b": {"__data__": {"id_": "f39eb7b3-9bbe-4dae-a82b-96b08c82a64b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1116637-ab73-489c-b220-d706da1e3e50", "node_type": null, "metadata": {}, "hash": "7ad3fedd4515814b48fcaa8740399ca79371559e71f7077d034abe8ee2b5758c"}}, "hash": "90f5c8a57b6c32d5a5b8ddfe11d307e6a6d2b8b42af354195e90b623b270cc78", "text": "- tags: #optical_flow #3d-reconstruction #event-based #unsupervised_flow \n- year: 2022\n- link: Not worth it\n- research group: Nantes Universit\u00b4e\n- contributions: Nothing literally\n- quantitative results: Shit\n- remarks: The paper is a copy of [[Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion]]\n- builds on: [[Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion]] but no build up\n- updated work: -", "start_char_idx": 0, "end_char_idx": 441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc87b453-d772-47d1-bc0d-b52c80082ca1": {"__data__": {"id_": "cc87b453-d772-47d1-bc0d-b52c80082ca1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68528a0f-c5e0-4bd0-b3ce-7d493340cf24", "node_type": null, "metadata": {}, "hash": "3cbff6ca295cfdb88dc06b3d3476d92cc441072b58f0b2de70596e07865525e6"}}, "hash": "79db2edafe5a31563e853f78a7214bc7edc90f09413ee8bd70a513441db22147", "text": "- tags: #event-based #optical_flow #unsupervised_flow #3d-reconstruction \n- year: 2022\n- link: http://www.iri.upc.edu/files/scidoc/2645-Event-transformer-FlowNet-for-optical-flow-estimation.pdf\n- research group: Institut de Rob\u00f2tica i Inform\u00e0tica\n- contributions:\n\t- RNN-ViT architecture\n- quantitative results: \n- remarks: Event count images were used, not sure why results are close to SOTA although the network architecture is much more complex. Probably event representation.\n- \n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f68946de-52bb-4526-a8d5-5960099baf68": {"__data__": {"id_": "f68946de-52bb-4526-a8d5-5960099baf68", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d5b6ac9-1a1a-4ef1-b3d5-182a5dee8d56", "node_type": null, "metadata": {}, "hash": "31d6de373a83cbba501170ea60fd7bf911592b7fc617f4fba3092031d2616750"}}, "hash": "bdd8680ad094c7f8b2b3c947374239d0d390eeb0f792baf28989f28a8b5b307f", "text": "- tags: #event-based #optical_flow #3d-reconstruction \n- year: 2021\n- link: https://arxiv.org/abs/2108.10552\n- research group: ETH Zurich\n- contributions:\n\t- First event camera optical flow\n\t- First method utilizes cost volumes on events\n\t- Real world dataset\n\t- Experiments\n- Architecture:\n- \nInput --> Voxel Grids\n- quantitative results: \n\n- remarks:\n\t- E-RAFT does not assume brightness consistency\n\t- Dense not sparse, so no specific features are tracked\n- builds on: RAFT\n- updated work:", "start_char_idx": 0, "end_char_idx": 492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb0466fa-131e-485c-a833-d2cd3fbcef6c": {"__data__": {"id_": "bb0466fa-131e-485c-a833-d2cd3fbcef6c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "005190b1-7690-4b09-9610-ec80d5def234", "node_type": null, "metadata": {}, "hash": "c921f503b472e3d5a5ed5cc38aa74416d5b66783fc10b2bbbffacf3e9ff9bcea"}}, "hash": "bdf636dfca64d3b5accdbb765639c7d6e7a6e3eb5957249da49458135d862d97", "text": "- tags: #event-based #optical_flow #unsupervised_flow #3d-reconstruction \n- year: 2018\n- link: https://arxiv.org/abs/1802.06898\n- research group: University of Pennsylvania\n- contributions:\n\t- An image-based representation of events\n\t- U-Net network architecture for self-supervised optical flow (photometric loss)\n- quantitative results: \n- \n- remarks: The loss function here is photometric loss from RGB images (DAVIS)\n- builds on: -\n- updated work: ET-FlowNet", "start_char_idx": 0, "end_char_idx": 462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bce798b2-2a4e-446e-a8ab-46908c6f5c79": {"__data__": {"id_": "bce798b2-2a4e-446e-a8ab-46908c6f5c79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10fe652f-188a-4851-b215-45f623ed6968", "node_type": null, "metadata": {}, "hash": "eaf155000fab4ca01adbb2fd83e627d077e278543f10a6b96941c5cf1282649a"}}, "hash": "81707022c6713e92102336121d48e328f5408f34491aa9038a35ee9bc59a0bf8", "text": "- tags: #event-based #3d-reconstruction #optical_flow\n- year: 2018\n- link: https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.pdf\n- research group: University of Pennsylvania\n- contributions:\n\t- Voxel grids\n\t- Improved loss to cmax\n\t- Stereo similarity loss\n\t- exps\n- quantitative results: \n\t\t\n- remarks: The loss is from event streams only, not photometric. The loss function is differentiable because there is no rounding when events are warped. Doesnt overfit. However, comparison is not available. \n\t- Warp:\n\t- \n\t- Loss:\n\t- \n\t- \n\t- The loss above will remove the scaling problem. This used to happen because if events are warped to reference time t1 initial events are of higher importance and vice versa. So T_p is computed forwards and backwards.\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c566e29a-3b32-464b-bf97-c02007bd1d41": {"__data__": {"id_": "c566e29a-3b32-464b-bf97-c02007bd1d41", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcc3daa5-9848-4b04-b22b-d70f46998c9b", "node_type": null, "metadata": {}, "hash": "9c6684861f1c901c4347c09ba4a99d03871800c785fdc370f6a44759d879499e"}}, "hash": "576752294aa77f4dc9d1f9f2a9158f39df5e4e1dfa305d0c9af35f5a25057fda", "text": "- tags: #frame-based, #algorithm, #simulation, #experimental, #3d-reconstruction, #optical_flow, #sim2real, \n- year: 2021\n- link: https://ieeexplore.ieee.org/document/9361253\n- research group: Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong\n- contributions:\n\t\t- Presenting a novel dense tracking pattern design and tracking algorithm to obtain an accurate and high-resolution 2D displacement field of the tactile surface (Optical flow)\n\t\t- Proposing a Gaussian density measure as a feature extraction method for dense tactile images (Improved dense representation)\n\t\t- Building a depth map processing pipeline that could estimate and refine the deformation depth in real-time at 40 Hz (flow --> depth)\n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a670611-b839-481d-b722-de5d76d071f3": {"__data__": {"id_": "0a670611-b839-481d-b722-de5d76d071f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26cec0a0-7740-4494-a32c-542a64074d9e", "node_type": null, "metadata": {}, "hash": "486226f1bcf3a89831ee23c3f108ba2ff1e266ed4f5f4142cd1b96663544b597"}}, "hash": "fcae2367908cedd941af543409bb63b345903ee25dbb17be9825a254a3b98ef1", "text": "- tags: #event-based #application #design\n- year: 2022\n- link:\n- research group: KUCAR\n- contributions: \n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63d76fa9-05f7-4929-9b27-9c0e5320d73f": {"__data__": {"id_": "63d76fa9-05f7-4929-9b27-9c0e5320d73f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c7c29b3-c610-4a9d-b017-da04dc149173", "node_type": null, "metadata": {}, "hash": "0e8da228781f3502944e42ce4b2dffa867dbc6469c66ed21a52a7e7a220537dd"}}, "hash": "0308e4689e3137222e63563ace15b96a2c9fbd33baaa98cc1f96519bb47ddadb", "text": "- tags: #neuro-vbts #event-based #experimental \n- year: 2020\n- link: https://ieeexplore.ieee.org/document/9197046\n- research group: bristol\n- contributions: neuromorphic hemispherical tactile sensor applied to texture recoginition. They train on encodings generated by different samples. The encodings are generated by tracking the markers' position and using information about events occuring within the vacinity of the markers. An example of such encoding is called the intensive encoding defined as: $$R=\\frac{1}{N} \\sum_{n=1}^N\\sum_{i=1}^{I_n} t_n^i + a$$ where N is the number of markers and $I_n$ is the number of events within the vacinity of marker $n.$ \n- quantitative results: \n\n- remarks: they do not provide an algorithm for tracking markers using the neuromorphic stream only.\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0e97d01-0a5b-4f0b-b3ad-5aa51014ae6e": {"__data__": {"id_": "e0e97d01-0a5b-4f0b-b3ad-5aa51014ae6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4f5885-091b-4cdb-b8ff-ccbd1e9a00a1", "node_type": null, "metadata": {}, "hash": "b2d28bdb30eceeb25c376a8c0561e63aa186f2ef3874e736e45c246ffa248fb6"}}, "hash": "bbd70e4bdce1baf2be48738810746583d421e92031e190c88d7e20bbc298b51c", "text": "- tags: #frame-based #experimental #algorithm \n- year: 2023\n- link: under review\n- research group: aric\n- contributions: this paper proposes a multifunctional sensor using both vision and tactile feedback. The vision is used to find the workpiece then the sensor closes itself allowing for tactile feedback to take place. This was done for normality detection. \n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "530b337d-6090-4f8d-a172-e21eddf7f338": {"__data__": {"id_": "530b337d-6090-4f8d-a172-e21eddf7f338", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3e94c23-c886-4da2-a54a-8ec59d5d4425", "node_type": null, "metadata": {}, "hash": "3d30286570e8cff4c8125c945489091cd646e28940bf8b26e1a0f43c2ad84e57"}}, "hash": "ddd45264b243a38db9dedd91664b92c99e06512f46106c9799274beba3edbcde", "text": "- tags: #event-based #algorithm #experimental #neuro-vbts \n- year: 2022\n- link: https://www.mdpi.com/1424-8220/22/18/6998\n- research group: bristol\n- contributions: snn for classification\n- quantitative results: \n- remarks: they train the SNN unsupervised\n- builds on: [[NeuroTac A Neuromorphic Optical Tactile Sensor applied to Texture Recognition]]\n- updated work:", "start_char_idx": 0, "end_char_idx": 366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58f7540f-87d2-46da-b8b7-d4199a3199f7": {"__data__": {"id_": "58f7540f-87d2-46da-b8b7-d4199a3199f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "385fc94f-a48c-4e65-93c5-a73ca8c2426c", "node_type": null, "metadata": {}, "hash": "30568df8a2bedf95724b73ebe4dcfefc0071736e85a9ae736e8e973f99597719"}, "3": {"node_id": "7ffe4dad-4ee7-43f9-8a3f-d2277e88524f", "node_type": null, "metadata": {}, "hash": "b1a1457af80d91fb93036b70909fcd08a5d99e19c4e7fe634c02b8dd8d156064"}}, "hash": "30b9df1cf8f2cb32ae487c7b331dd8df4f60d1bb93baa636223ed5e354ae6de7", "text": "- tags: #algorithm \n- year: 2023\n- link: https://arxiv.org/pdf/2302.04181.pdf\n- research group:\n- contributions: review paper on graph transformers\n- \n- quantitative results: \n- remarks: \nGeneral node level attention can be realized as a GNN where messages are aggregated in a permutation invariant manner from _all_ the graph \nSince no structuctural encoding or information embedded in the above formulation, this model performas equivalently to a DeepSets model. i.e. there is no iformation about the structure of the graph or the geometry. Thus either positional or structural information myust be encoded in the attention model. This paper goes into details about different encoding needed,\n\nTje above formulation attends over individual nodes. However, in the case of graphs constructed from events, this can be costly. Thus the authors e identify three approaches to graph tokenization: (1) nodes as tokens, (2) nodes and edges as tokens, and (3) patches or subgraphs as tokens. Operating on the nodes level has quadratic complexity O(n^2) since an n by n attention matrix needs", "start_char_idx": 0, "end_char_idx": 1084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7ffe4dad-4ee7-43f9-8a3f-d2277e88524f": {"__data__": {"id_": "7ffe4dad-4ee7-43f9-8a3f-d2277e88524f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "385fc94f-a48c-4e65-93c5-a73ca8c2426c", "node_type": null, "metadata": {}, "hash": "30568df8a2bedf95724b73ebe4dcfefc0071736e85a9ae736e8e973f99597719"}, "2": {"node_id": "58f7540f-87d2-46da-b8b7-d4199a3199f7", "node_type": null, "metadata": {}, "hash": "30b9df1cf8f2cb32ae487c7b331dd8df4f60d1bb93baa636223ed5e354ae6de7"}}, "hash": "b1a1457af80d91fb93036b70909fcd08a5d99e19c4e7fe634c02b8dd8d156064", "text": "complexity O(n^2) since an n by n attention matrix needs to be computed. \n- builds on:\n- updated work:", "start_char_idx": 1028, "end_char_idx": 1130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "758a5dc7-4a9f-4ac4-bc01-246e743231b3": {"__data__": {"id_": "758a5dc7-4a9f-4ac4-bc01-246e743231b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0fcab45-e0b9-4042-9171-53ee21293747", "node_type": null, "metadata": {}, "hash": "2f4baef1545df0662a666963d1c47d23ffcd27a43dd3a42592381fdc0823d704"}}, "hash": "7f4bd9409fd5990aa0c42b8e2bcff9a18e0c108aaff9cb5275abf23c81e1bc32", "text": "- tags: #\n- year: \n- link:\n- research group:\n- contributions:\n- quantitative results: \n- remarks:\n- builds on: [[AEGNN Asynchronous Event-based Graph Neural Networks]]\n- updated work:", "start_char_idx": 0, "end_char_idx": 183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01031466-5c9c-423f-b7f3-fd8daea93440": {"__data__": {"id_": "01031466-5c9c-423f-b7f3-fd8daea93440", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e39865cb-1fbf-4c65-947c-cbd38fd0141b", "node_type": null, "metadata": {}, "hash": "8899e1c1ef0ca52329e14930a37078762542b48dc9a2aeac3da29332668e47f3"}}, "hash": "3f37da2242a9e2aa3086b183f017a9181bc26323f98b8fa834bef3c6bfbd7bd2", "text": "- tags: #frame-based #simulation #experimental\n- year: 2022\n- link: https://www.mdpi.com/2073-4360/14/23/5097\n- research group: aric\n- contributions: sim2real study for vision-based tactile sensor for normality detection\n- quantitative results: from 0.375-0.425 mm error between simulation and experimental results. This was done on angles of contact from 0-12 degrees.\n- remarks: \n- builds on: same sensor as [[A Novel Vision-based Multi-functional Sensor for Normality and Position Measurements in Precise Robotic Manufacturing]] \n- updated work:", "start_char_idx": 0, "end_char_idx": 548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e2fcdd68-a88e-493c-8bf9-80f4819c3871": {"__data__": {"id_": "e2fcdd68-a88e-493c-8bf9-80f4819c3871", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "498af3cf-4320-4885-953d-7ba32db1c6b5", "node_type": null, "metadata": {}, "hash": "dbdf7ca67d07563eb03c8fe71d5c10b91ced7949fab4d6bf9d5a6d60635e5725"}}, "hash": "27c230472de1184ea14d9b919f4ba26c1909fd536e0331100e16dbcaa4b94969", "text": "$$C=\\sqrt(\\frac{E}{(1-\\mu^2)\\rho}=\\sqrt(\\frac{40000}{1050(1-0.42^2)}=6.8 m/s$$\n$$\\Delta t = \\frac{L}{C}=\\frac{L}{6.8}=0.15L$$\n#Courant_number", "start_char_idx": 0, "end_char_idx": 141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e440a22-d429-400a-9629-61894ca41a74": {"__data__": {"id_": "4e440a22-d429-400a-9629-61894ca41a74", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e143ce2-864f-41be-96f1-5a0bf69de35f", "node_type": null, "metadata": {}, "hash": "aa76f3c721b6c4233e6ee78a08c81697a8b781f7a4ac62ae1b2727da78d2ae96"}}, "hash": "73ef5523d868e0de003fc53d9bf5c225b11a04c824f90aff60ff275e2492a3d9", "text": "- tags: #material #simulation \n- year: \n- link: https://www.theses.fr/2021UCFAC103.pdf, https://www.researchgate.net/scientific-contributions/Amir-Pagoli-2184119011\n- research group: \n- contributions: \n- quantitative results:  \n- remarks: \n- builds on: \n- updated work: \n\n\nhe explored almost all the Hyperelastic martials that could be used for the tactile sensors or soft robotics in general.", "start_char_idx": 0, "end_char_idx": 393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "991e4599-295d-4c87-a7dc-95bf18328d07": {"__data__": {"id_": "991e4599-295d-4c87-a7dc-95bf18328d07", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "339615d0-f5d0-433d-9ac0-306877a93a7b", "node_type": null, "metadata": {}, "hash": "e684a9f8ebf9dd8d3de4825764c943541d6f031011e34a231f793631e4869f3b"}}, "hash": "9fc67e4ccce7777e3d8575c493a38b652d4a8a0acb4e9f4c934fb60f8abff8db", "text": "chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://indico.cern.ch/event/801141/contributions/3329395/attachments/1817203/2970579/2019_02_25_Explicit_codes.pdf\n\n#Courant_number", "start_char_idx": 0, "end_char_idx": 184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e625d39-4fbd-484e-ba9d-d570fec90736": {"__data__": {"id_": "0e625d39-4fbd-484e-ba9d-d570fec90736", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07de0b85-5d5c-4e9e-bb0f-5229ae05e54f", "node_type": null, "metadata": {}, "hash": "be04bece0bc8b7273a8b3283d66982f8cbbc2b80d7f0574e183d58c54dc7c7f9"}}, "hash": "b87bb3f0b08f96e978ee6471e892bb9baf2b1123540ddafd5f2992e78070b66f", "text": "- tags: #material_model\n- year: 2018`\n- link: https://onlinelibrary.wiley.com/doi/abs/10.1002/app.47025\n- research group: \n- contributions: present new material model and compare it with the previous models\n- quantitative results: \n- remarks: the proposed model helps to get more accurate results for the large strains cases. Our tactile sensor works in a very low strain range. So, we can consider any of them.\n- builds on: \n- updated work: \n\n\nMaterial Model -> Yeoh (incompressible)\n| Parameter | Value |\n|-|-|\n| C1 | 17000 |\n| C2 | -200 |\n| C3 | 23 |", "start_char_idx": 0, "end_char_idx": 553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f380dc30-8c8c-47ee-9dc5-6a9905a6686b": {"__data__": {"id_": "f380dc30-8c8c-47ee-9dc5-6a9905a6686b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e4959f8-9e24-4bdd-a252-97dcdc6635b0", "node_type": null, "metadata": {}, "hash": "b36a4ea1d0cce7c57dc2f017f3f468851ee8bc9f49aeb883b1d167c2b116af89"}}, "hash": "c66135580a0c97dd8b6f33ea15d4430d2492964624485352b33e4f1873148dee", "text": "- tags: \n- year: \n- link:\n- research group:\n- contributions:\n- quantitative results: \n- remarks:\n- builds on:\n- updated work:\n- Code or dataset:\n- Ideas or extension on the work:", "start_char_idx": 0, "end_char_idx": 178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96d682e8-cb07-487c-a02f-37529fbf6dd7": {"__data__": {"id_": "96d682e8-cb07-487c-a02f-37529fbf6dd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cb94f33-d107-4f9f-9011-72cea3304c74", "node_type": null, "metadata": {}, "hash": "600ed71744c04db284eb47f4ffe22de4fc9716167b22d64af25547e133494f97"}, "3": {"node_id": "97237e60-5ade-426b-9f8b-05836002b1b3", "node_type": null, "metadata": {}, "hash": "e18cfe34dd5422cdabb358a63507dfeb979443f3208bdb25af524c2d67257678"}}, "hash": "9b0a57a21fa61974675b77890437b420e46862653047aa877e6de19a98355a07", "text": "This is a sharepoint sync solution for ubuntu users. If you find anything else easier than this, please add it here.\n\nYou will need to install rclone from https://rclone.org/downloads/\n```bash\nwget https://downloads.rclone.org/rclone-current-linux-amd64.deb\nsudo apt install ./rclone-current-linux-amd64.deb\n```\nconfigure it via\n```bash\nrclone config\n```\nadd a new remote by pressing 'n', call it \"aric.\" Choose \"Microsoft OneDrive\" by typing 31 and hitting enter. Hit enter twice without typing anything for OAuth. Choose \"Microsoft Cloud Global\" by typing 1 and hitting enter. Do not edit the config, press 'n.' Press enter to authenticate via browser. Log in to your Microsoft KU account. Press 3 for sharepoint \"Sharepoint site name or URL.\" When prompted to enter site name paste\n```\nhttps://kuacae.sharepoint.com/sites/IntelligentRoboticManufacturing\n```\nPress 'y' and enter. 'y' again; do not edit anything. Quit the config by hitting 'q'.\n\nThe above steps need only need to be done once. From now on, if you want to mount the sharepoint, type in the", "start_char_idx": 0, "end_char_idx": 1057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97237e60-5ade-426b-9f8b-05836002b1b3": {"__data__": {"id_": "97237e60-5ade-426b-9f8b-05836002b1b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cb94f33-d107-4f9f-9011-72cea3304c74", "node_type": null, "metadata": {}, "hash": "600ed71744c04db284eb47f4ffe22de4fc9716167b22d64af25547e133494f97"}, "2": {"node_id": "96d682e8-cb07-487c-a02f-37529fbf6dd7", "node_type": null, "metadata": {}, "hash": "9b0a57a21fa61974675b77890437b420e46862653047aa877e6de19a98355a07"}}, "hash": "e18cfe34dd5422cdabb358a63507dfeb979443f3208bdb25af524c2d67257678", "text": "be done once. From now on, if you want to mount the sharepoint, type in the following command to mount the sharepoint to '~/OneDrive'\n```bash\nrclone --vfs-cache-mode writes mount aric: ~/OneDrive\n```\n keep this command running while using obsidian to keep the syncing going. Open obsidian and choose ~/OneDrive/Literature as your vault. You can add this command to your startup routine if you want it to be always syncing. \n If anything goes wrong and you want to force an unmount\n```bash\nfusermount -uz ~/OneDrive/\n```", "start_char_idx": 982, "end_char_idx": 1501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db2966a7-8354-40d1-9686-4132769d315c": {"__data__": {"id_": "db2966a7-8354-40d1-9686-4132769d315c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75a21967-0a6f-4ebe-bf81-3a71f06c60b5", "node_type": null, "metadata": {}, "hash": "8b1f2179eb7fdeaa247e51f515d57da1b5f482dd6375834bbae377061411f3e6"}}, "hash": "b07d00c9a50b2a8b469c4349670ca13ea100eaae9a6c863080cd97f2bfc8a5bb", "text": "- tags: #event-based #SNN #classification #neuromorphic_hardware #Loihi2-chip \n- year: 2023\n- link: Flow Cytometry With Event-Based Vision and Spiking Neuromorphic Hardware (thecvf.com)\n- research group: Ghent University -imec\n- contributions: - binary classification with event-based vision and spiking neuromorphic hardware\n\t\t\t\t\t\t-  Improve SNN and its learning algorithm to make it more performant and hardware compatible.\n\n- quantitative results: - Demonstrated a neuromorphic advantage in terms of accuracy, as none of the tested ANN using frames outperformed our spiking neural network trained directly on events from the event-based camera\n- remarks: - sampling rate of 10 microsecond \n\t\t\t\t- running on Loihi 2 intel chip\n- builds on:\n- updated work:\n- Code: https://github.com/stevenabreu7/dvs_flow\n- Ideas: -M.Halwani: This would be a great start for us to test neuromorphic chips and SNNs", "start_char_idx": 0, "end_char_idx": 898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a179080-2643-4c5f-b8fa-0f5fb4a3fac6": {"__data__": {"id_": "0a179080-2643-4c5f-b8fa-0f5fb4a3fac6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38f0359a-78c3-45d0-9c7a-197c1bf30cef", "node_type": null, "metadata": {}, "hash": "69af9bf0e1294bb032ae7278c81fabd17063ffc7076c6864890d158edac494ff"}}, "hash": "9e3e3d5fc4581f8d1c4ac515b4db84db13c0beafb034a79860867fc42b2f1442", "text": "Literature Review\nThe paper uses Deep Q-Network to graps objects in a clutered scene. They use a novel gripper design where a gripper is equipped with a suction module in the middle. This is done to grasp objects first with the suction gripper then establish a better hold with the gripper. They used an affordance map, which analyzes the gripping points in the scene. The reward function is based on this affordance map, the objective of RL is to check the if the affordance map generated is good. otherwise the objects are moved to generate a new affordance map. The actions here are pushing the object.\n\n__Research idea__ generate an affordance map from events, steroe events or with an events camera.\n\n---", "start_char_idx": 0, "end_char_idx": 709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "18b5855a-da3d-4584-824d-e19bcd3bba44": {"__data__": {"id_": "18b5855a-da3d-4584-824d-e19bcd3bba44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04d7ebc5-25bc-44b9-8989-8b22762e1bcb", "node_type": null, "metadata": {}, "hash": "63b8dd1a82e12f814750f64bffc6ba1553e36001921421552c3dc7f7245421dc"}}, "hash": "b7f6e76a9f53cce566c69773c5f00d01763dfb1e8cfd678c6fe729191b8ba656", "text": "__Learning Target-Oriented Push-Grasping Synergy in Clutter With Action Space Decoupling__ (RAL 2022)\n\nThey address the problem of training in a large state space by training two policies. a position net and an angle net. This is able to learn in a cluttered environment and perform push-grassp strategies.Failure in grasp strategies is usually from collision between the gripper and the surrounding objects.", "start_char_idx": 0, "end_char_idx": 408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a3374b56-7286-43c9-98f7-519553572b5b": {"__data__": {"id_": "a3374b56-7286-43c9-98f7-519553572b5b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc24781d-5656-4316-9a7d-cc588e50d7bc", "node_type": null, "metadata": {}, "hash": "a469fb977f106cd932bb27bdf6289d7dc51b87b026e7f1040a3832bb2cb7945c"}}, "hash": "926afdc7d559c93555ab997074f4e817ce5813e3b7c5a79d0c84d23f13b0be41", "text": "Tactile Sensing with RL\n\nThe work investigates sim-to-real between tactile sensors usuage in simulation and the real world. They represent high-resolution contact geometry as depth images. some advantages of using simulation is avoiding damage during exploratory training. exploiting privileged information from the sensor. tactile sensors advatanges are that they do not suffer from occlusion, contact information can be more detailed than visual images of an entire scene and observations are constrained. Domain randomisation was used to on simulated images to train a generator robust enough to generalise to real visual images. one novelty in their method is that since they use depth images. the model and training is agnistic to the tactile sensor being used. They simulate in pybullet with rigid bodies with soft contacts as an approximation to the deformation of real tactile sensors. Adding texture to the simulator may improve the photo-realistic image generation.\n\n---", "start_char_idx": 0, "end_char_idx": 980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "df9f576c-0cd5-41b2-a0d8-fab0e1950c79": {"__data__": {"id_": "df9f576c-0cd5-41b2-a0d8-fab0e1950c79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e374ceb0-c889-433f-ba89-d157a81e2013", "node_type": null, "metadata": {}, "hash": "67b8ef0274bdf2005aa4cb80a973fce35862ce293a75563ef8eb90f1de6a9dd8"}}, "hash": "89f633f3a886e13852f91ccc1ecdda503321866cdc0eba3b4ee8c47c4d2b3ec2", "text": "Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch\nThe work investigates bridging the sim to real gap by training a simulator using an RL policy for following an edge (among other different applications). They claim they achieve zero-shot performance on multiple exploration and manipulation tasks using three tactile sensors mainly (TacTip-DIGIT-DigiTac) TacTip and DigiTac are marker based. The sim to real gap can be explored using two main approaches 1) using FEM analysis to model the sensor dynamics 2) or using the image rendering method to replicate the sensory data. The tactile information was rendered as a depth image. GANs were used to train a model to bridge the gap between simulated and real images. PPO was used as the policy to train the agent. DIGIT type tactile sensor failed on a concave surface due to its shape.\n\n---", "start_char_idx": 0, "end_char_idx": 896, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "47fdb869-eeb8-463b-ae30-688374ae0b26": {"__data__": {"id_": "47fdb869-eeb8-463b-ae30-688374ae0b26", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96cf3362-3210-436c-b929-54fb78fde30b", "node_type": null, "metadata": {}, "hash": "e84de6517825acc133ec3db8e697853666e7ae713880a13987cd3c4387f9043a"}}, "hash": "5ef517a532efb8c9f07ea324e35ed23bff62d552f64a055be77d2a8d6e15b597", "text": "Tactile-RL for Insertion: Generalisation to Objects of Unknown Geometry\nThis work investigates object insertion using tactile and reinforcement learning. Tactile sensors co-located at the gripper fingers are better positioned to capture contact events than vision sensors, which suffer from occlusions and limited accuraccy. They used real experiments to train the agent and used F/T sensors based on the fact that it generalises well to objects of different geometry. The agent was trained for 8 hours in 500 episodes. The optimal RL policy tries to achieve the maximum reward by inserting the object inside the hole within a sequence of actions. The clearnace between the object and the experiment was around 3mm. using vision only would result in noisy position of the hole. The action is the continous robot displacement in (delta x, delta y and delta theta). The reward is based on the contact error. DDPG was used as the learning algorithm. RL with RGB, RL with Force torque feedback. The method tends to overfit when using RGB data from the tactile sensor, maybe using transformers and more data augmentation techniques would help.\n\n---", "start_char_idx": 0, "end_char_idx": 1143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ddf20be-db42-4c26-9b29-78da837c1715": {"__data__": {"id_": "1ddf20be-db42-4c26-9b29-78da837c1715", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "258e7ffd-9b5f-44c0-822c-e1e494ee93a0", "node_type": null, "metadata": {}, "hash": "a153dd8569783e1e0822862f77e6e720ec343d487b62ff403a19b0dc831f27d9"}}, "hash": "c4c0d6d82676589a4a446df1260b189e955bff53c016e88fcf747665921b2837", "text": "Visuotactile-RL: Learning Multimodal Manipulation Policies with Deep Reinforcement Learning\nThis paper investigates the use of two modalities along with the proprio encoder for the task of reaching an object, opening a door and lifting a cube. They talk about how most works do not leverage moren pixel-based tactile sensors with external sensors. they consider a challenging triad: dexetrous manipulation tasks, high resolution visual and tactile sensors and reinforcement learnring. They try to tackle the discontinous nature of physical interactions. Their main question in the main is how to prevent the agent from over-biasing its attention on visual feedback. and do the discontinuities negatively affect the learning stability. They introduce something called tactile gating which prevents the flow tactile feedback in the absesnce of detected contact. DrQ agent was used as it learns directly from images. DrQ is special in that it does augmentation to the images during training. The authors also discuss degrading the visual signal to imporve multimodal performance. \n\n---", "start_char_idx": 0, "end_char_idx": 1082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ccf64be1-25a9-4da9-b6d7-f694a3a3e988": {"__data__": {"id_": "ccf64be1-25a9-4da9-b6d7-f694a3a3e988", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbdeb71b-afda-4635-b4ab-fca1b6552ebf", "node_type": null, "metadata": {}, "hash": "bdfa3e1240dfdc99c7885f334ebbf2744380933fd669a3b4dae0f41949e07019"}}, "hash": "bfac2612f011c92ed5135830f430f9082eaf85b38855d0b4599b28c0c0613a7a", "text": "More than a Feeling: Learning to Grasp and Regrasp using Vision and Touch\n\nThey propose an end-to-end acition conditional model that learns regrasping policies from raw viso-tactile data. Their work also tries to optimize the minimum force required to grasp the object. They can grasp novel objects. a CNN is used to extract features and fuse the two modalities with a MLP at the end. there is no mention of the agent used to train, just that it was an on-policy model. The action is a 5-dimensional vector consisting of a  3D motion, in-plane rotation, and change in force. They baseline against other simple approahces that uses depth data and calculate the centroid of the object. Future work is to extend this to a more cluttered environement. A more fine actions might produce better results. The biggest improvement in grasping were in objects that are difficult to ascertain a good grasp.\n\n---", "start_char_idx": 0, "end_char_idx": 900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bdebe034-3c1c-48a8-a506-330c5fe931aa": {"__data__": {"id_": "bdebe034-3c1c-48a8-a506-330c5fe931aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81e23f77-902f-41e0-bf0b-c1e86cb25b2c", "node_type": null, "metadata": {}, "hash": "a93e88be29e0d935d344eccacbf7aa03db5e1af09cffad6a1cb918af08a2a2d7"}}, "hash": "54d9b6f6fdc1b64f4167d181ad66663e1bcf89f08566447b682c2af9f4437641", "text": "Tactile-Based  Insertion  for  Dense  Box-Packing\n\nThe main contribution of this work is that they try to insert an object into a box. They use the gelsight sensor to estimate the error in the insertion with the presence of other objects in the box. The object held when colliding with other objects produces direction and a error magnitude which help the robot to correct for it. A CNN + LSTM is used to process the images. Since the controller then designed is heurostic, it would much more ideal if we could take the raw tactile measurments and pass them as observations to the RL agent to find the best wa to insert the object in a cluttered scene.", "start_char_idx": 0, "end_char_idx": 652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4d7297a0-a687-4234-af3a-820f144a0af3": {"__data__": {"id_": "4d7297a0-a687-4234-af3a-820f144a0af3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd55eee-e366-4973-901f-d974b47b8314", "node_type": null, "metadata": {}, "hash": "25a2bef8b5623c9c957711fbadb24ae5963561c2833d3aff8f000e44cdfdd1e8"}}, "hash": "032f4f61dd1a10490e94348b96692bd2920b7aa7e79aad294e360b0b50523751", "text": "- tags: #RL #pickAndPlace \n- year: \n- link:\n- research group:\n- contributions:\n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a76e208-2432-4f0c-8391-01a8c9afb239": {"__data__": {"id_": "0a76e208-2432-4f0c-8391-01a8c9afb239", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12ae4899-a053-4ee3-8892-ab9c758e5361", "node_type": null, "metadata": {}, "hash": "bffc52f8b6af03a4636b6f8795094559c9dd9ac40b9158a545dce96657ab5f2b"}, "3": {"node_id": "97c7d7f6-5331-4672-9313-526ed8a7f5e6", "node_type": null, "metadata": {}, "hash": "ef0bcb7d71677c35db53155b6d5891e4cadc8c3d914297055c974999c2d8ac83"}}, "hash": "76b09cb0b1532b75bd746f62c35f062ed586df31e15ba7273aa302fe99859b89", "text": "- tags: #event-based \n- year: 2022\n- link: https://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sabater_Event_Transformer._A_Sparse-Aware_Solution_for_Efficient_Event_Data_Processing_CVPRW_2022_paper.pdf https://github.com/AlbertoSabater/EventTransformer\n- research group: Dr. Ana C. Murillo, Zaragoza \n- contributions: A cheap transformer-based architecture for event-based tasks that is runable on a CPU. The authors propose a sparse ViT-based approach of processing event streams. They also propose a sparse patches approach. This  reduces computations by quite a lot as can be seen in the table below  Where the average number of activated patches is displayed. It is much <50% lower than all the possible patches. A patch size of 6x6 is used. This allievates the quadratic complexity of attention based methods. Further more they use Perciever based attention http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf.\n- quantitative results: RG-CNN", "start_char_idx": 0, "end_char_idx": 960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97c7d7f6-5331-4672-9313-526ed8a7f5e6": {"__data__": {"id_": "97c7d7f6-5331-4672-9313-526ed8a7f5e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12ae4899-a053-4ee3-8892-ab9c758e5361", "node_type": null, "metadata": {}, "hash": "bffc52f8b6af03a4636b6f8795094559c9dd9ac40b9158a545dce96657ab5f2b"}, "2": {"node_id": "0a76e208-2432-4f0c-8391-01a8c9afb239", "node_type": null, "metadata": {}, "hash": "76b09cb0b1532b75bd746f62c35f062ed586df31e15ba7273aa302fe99859b89"}}, "hash": "ef0bcb7d71677c35db53155b6d5891e4cadc8c3d914297055c974999c2d8ac83", "text": "quantitative results: RG-CNN [[Graph-Based Object Classification for Neuromorphic Vision Sensing]] beats it in the long sequences test. Maybe this is because ev-tranformer does not have a temproal module.\n- \n- remarks: the benchmarks are done on old datasets like ASL-DVS and N-Caltech. Akk of these are relatively small datasets with sequences lasting 6s max.\n- builds on:\n- updated work:", "start_char_idx": 932, "end_char_idx": 1321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c99391ae-b035-410b-bc27-d8f44176a2ac": {"__data__": {"id_": "c99391ae-b035-410b-bc27-d8f44176a2ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c390adb4-8ec2-4730-aef4-45823777b1df", "node_type": null, "metadata": {}, "hash": "aeaad0a5748b7263ea7850372728ffd4e72de5d75e49552c8d887b4873519b6b"}, "3": {"node_id": "df1823a8-f3da-4d8a-be8b-d1ccd0a5aef1", "node_type": null, "metadata": {}, "hash": "5eb41f11183fe4f7f12513138f413857d860b3f4325be7aa3a5a671aabcb9cc6"}}, "hash": "42d192ce83d8d04eeef094eeefadc6e71016d4c11fee40f5e209d0bb38bdda7f", "text": "0- tags: #event-based #algorithm \n- year: 2023\n- link: https://arxiv.org/pdf/2212.05598.pdf\n- research group: scaramuzza\n- contributions: ViT for spatial and small temporal information, recurrence for large temporal information\n- quantitative results: they beat aegnn [[AEGNN Asynchronous Event-based Graph Neural Networks]], ASTMNet on object detection tasks on 1Mp and Gen1. As well as  achieving record low runtime of 3 ms only!! \n- remarks: \n\t- \"Differently from prior work [26,38] we find that temporal and spatial feature aggregation can be completely separated. This means that we use plain LSTM cells such that the states of the LSTMs do not interact with each other. By avoiding Conv-LSTM units [44], we can drastically reduce the computational complexity and parameter count\" this is interesting and goes against a lot of previous work. it could guide our intuition when designing new algos. Spatial and temporal features are extracted separtaly.\n\t- The use of torch.compile gave them a x3 fold decrease in computation time\n- builds", "start_char_idx": 0, "end_char_idx": 1042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "df1823a8-f3da-4d8a-be8b-d1ccd0a5aef1": {"__data__": {"id_": "df1823a8-f3da-4d8a-be8b-d1ccd0a5aef1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c390adb4-8ec2-4730-aef4-45823777b1df", "node_type": null, "metadata": {}, "hash": "aeaad0a5748b7263ea7850372728ffd4e72de5d75e49552c8d887b4873519b6b"}, "2": {"node_id": "c99391ae-b035-410b-bc27-d8f44176a2ac", "node_type": null, "metadata": {}, "hash": "42d192ce83d8d04eeef094eeefadc6e71016d4c11fee40f5e209d0bb38bdda7f"}}, "hash": "5eb41f11183fe4f7f12513138f413857d860b3f4325be7aa3a5a671aabcb9cc6", "text": "gave them a x3 fold decrease in computation time\n- builds on:\n- updated work:", "start_char_idx": 985, "end_char_idx": 1062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e1c69f8d-2abd-4eb3-a321-af818bf56394": {"__data__": {"id_": "e1c69f8d-2abd-4eb3-a321-af818bf56394", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0d6fc45-5bf0-47b4-8aa8-458b8f9af142", "node_type": null, "metadata": {}, "hash": "1f926cd31365e888636d3276a86984de5a40371e80b083ebcdc37e0cc19a2bec"}}, "hash": "b6fb638bac02824d4d7184741344e3448973859ddc0e08045bb62548f48b09b2", "text": "tags: #event-based \nyear: \nlink:\nsummary: builds on [[AEGNN Asynchronous Event-based Graph Neural Networks]]", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e788d27-0441-4418-8390-87bbf0ee3e47": {"__data__": {"id_": "6e788d27-0441-4418-8390-87bbf0ee3e47", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be3639b8-96ad-44fb-a5b9-d0e5326a546f", "node_type": null, "metadata": {}, "hash": "c5355ea671ae09b15243cca627d58da6e0a0d59a221f1a9aedc6f0c226c706b4"}}, "hash": "4922c31477aea559f67d89a6ef84d3b2dffbd9e6402a0cb7ac86a2463c2814a2", "text": "- tags: \n- year: \n- link:\n- research group:\n- contributions:\n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5dc1a04d-91f1-4904-96db-bbc4cc567810": {"__data__": {"id_": "5dc1a04d-91f1-4904-96db-bbc4cc567810", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4de7f4fa-cf73-41cd-9ecd-96fb3e2a0bb2", "node_type": null, "metadata": {}, "hash": "4c11d7247321efd855a4ec0a318f41025e5385d01cd32cac3ba6a7f6fd08a999"}}, "hash": "18df7c862c4d4109f255aaea49c2c58b6aada3eb725862c1628a3786900d5a3c", "text": "tags: #event-based \nyear: 2019\nlink: https://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.pdf\nsummary:", "start_char_idx": 0, "end_char_idx": 189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "754ce08f-2ebb-4ea8-aa43-9be63b847d82": {"__data__": {"id_": "754ce08f-2ebb-4ea8-aa43-9be63b847d82", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecff2333-5b06-4ee3-883f-26b9a93db345", "node_type": null, "metadata": {}, "hash": "3889ae4d444a6ea07431d76802b8a1c5e72a20e9a771d12c950d28075ba6f5f4"}}, "hash": "f27c989309f81386a3bdf0da422eb2a49ca5ad5808b191ef521b20efb31b6836", "text": "- tags: #event-based #algorithm \n- year: \n- link:\n- research group:\n- contributions:\n- quantitative results: \n- remarks:\n- builds on:\n- updated work:", "start_char_idx": 0, "end_char_idx": 149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d0e65c3-7a80-44b8-b0c2-edb997716aad": {"__data__": {"id_": "0d0e65c3-7a80-44b8-b0c2-edb997716aad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9255197-4bd9-4166-a132-dfa29d9a3f72", "node_type": null, "metadata": {}, "hash": "3918c9a87dd47a7cd71c329453627a54a2ec221a5a8d88a61c5e15cb83b638e6"}, "3": {"node_id": "6b024771-a7d8-487f-b6b4-97a86fde813d", "node_type": null, "metadata": {}, "hash": "10fe891e4549b08ff83afc596ba9dfd5a99558d48682d86463ea0b4a22b157d0"}}, "hash": "96895fb3e306ace025f552e4be488297399bbed4210ced86723fe5ce40f2acaf", "text": "- tags: #event-based #depth-estimation #neuromorphic_hardware #monocular #multiview-stereo #Loihi2-chip\n- year: 2023\n- link: Low-Latency Monocular Depth Estimation Using Event Timing on Neuromorphic Hardware (thecvf.com)\n- research group: Intel Labs\n- contributions: -event-based depth estimation method with a single DVS using a novel depth --  from motion algorithm targeting neuromorphic hardware\n- methodology: The system first computes the optical flow on the neuromorphic chip and then computes the depth by combining optical flow with the camera velocity. The method assumes only translational motion and it successfully reconstructs the depth from the measured flow\n- quantitative results: The method can achieve low-latency depth estimation (<0.5ms) while maintaining a small network size, allowing for better scalability.\n- remarks: -They use Loihi 2 chip.\n\t\t\t\t- Typically, spikes carry only binary information, but on Loihi 2 spikes can also carry a value. \n\t\t\t\t- They use this feature to send the measured time difference as a single spike, allowing for efficient encoding and low latency. The time", "start_char_idx": 0, "end_char_idx": 1110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b024771-a7d8-487f-b6b4-97a86fde813d": {"__data__": {"id_": "6b024771-a7d8-487f-b6b4-97a86fde813d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9255197-4bd9-4166-a132-dfa29d9a3f72", "node_type": null, "metadata": {}, "hash": "3918c9a87dd47a7cd71c329453627a54a2ec221a5a8d88a61c5e15cb83b638e6"}, "2": {"node_id": "0d0e65c3-7a80-44b8-b0c2-edb997716aad", "node_type": null, "metadata": {}, "hash": "96895fb3e306ace025f552e4be488297399bbed4210ced86723fe5ce40f2acaf"}}, "hash": "10fe891e4549b08ff83afc596ba9dfd5a99558d48682d86463ea0b4a22b157d0", "text": "as a single spike, allowing for efficient encoding and low latency. The time difference is then used together with the camera velocity, which is assumed to be known, to calculate the distance to objects in the scene\n\t\t\t\t- They presented a lightweight and low-latency method for depth estimation that can run on the Intel Loihi 2 neuromorphic chip. It can process the data event by event and produce depth measurements with a theoretical latency of 1ms or lower. The method currently has some limitations on the types of motions supported, limited only to translational motion. The overall accuracy is lower when compared to other monocular CPU-based methods that consider larger windows of events. However, it achieves lower latency and power consumption running on neuromorphic hardware\n- builds on:\n- updated work:\n- Code or dataset:\n- Ideas or extension on the work:", "start_char_idx": 1034, "end_char_idx": 1903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"01feec41-2935-4ad1-ac8f-17354b789117": {"node_ids": ["38570b03-279d-4e85-a9bc-6ec240d656e9"], "metadata": {}}, "900ace75-20ab-4fdf-8cbc-e3ce8d88e19f": {"node_ids": ["ec11c14b-8114-4905-9939-2770c915d9f4"], "metadata": {}}, "a1116637-ab73-489c-b220-d706da1e3e50": {"node_ids": ["f39eb7b3-9bbe-4dae-a82b-96b08c82a64b"], "metadata": {}}, "68528a0f-c5e0-4bd0-b3ce-7d493340cf24": {"node_ids": ["cc87b453-d772-47d1-bc0d-b52c80082ca1"], "metadata": {}}, "8d5b6ac9-1a1a-4ef1-b3d5-182a5dee8d56": {"node_ids": ["f68946de-52bb-4526-a8d5-5960099baf68"], "metadata": {}}, "005190b1-7690-4b09-9610-ec80d5def234": {"node_ids": ["bb0466fa-131e-485c-a833-d2cd3fbcef6c"], "metadata": {}}, "10fe652f-188a-4851-b215-45f623ed6968": {"node_ids": ["bce798b2-2a4e-446e-a8ab-46908c6f5c79"], "metadata": {}}, "fcc3daa5-9848-4b04-b22b-d70f46998c9b": {"node_ids": ["c566e29a-3b32-464b-bf97-c02007bd1d41"], "metadata": {}}, "26cec0a0-7740-4494-a32c-542a64074d9e": {"node_ids": ["0a670611-b839-481d-b722-de5d76d071f3"], "metadata": {}}, "6c7c29b3-c610-4a9d-b017-da04dc149173": {"node_ids": ["63d76fa9-05f7-4929-9b27-9c0e5320d73f"], "metadata": {}}, "2a4f5885-091b-4cdb-b8ff-ccbd1e9a00a1": {"node_ids": ["e0e97d01-0a5b-4f0b-b3ad-5aa51014ae6e"], "metadata": {}}, "a3e94c23-c886-4da2-a54a-8ec59d5d4425": {"node_ids": ["530b337d-6090-4f8d-a172-e21eddf7f338"], "metadata": {}}, "385fc94f-a48c-4e65-93c5-a73ca8c2426c": {"node_ids": ["58f7540f-87d2-46da-b8b7-d4199a3199f7", "7ffe4dad-4ee7-43f9-8a3f-d2277e88524f"], "metadata": {}}, "d0fcab45-e0b9-4042-9171-53ee21293747": {"node_ids": ["758a5dc7-4a9f-4ac4-bc01-246e743231b3"], "metadata": {}}, "e39865cb-1fbf-4c65-947c-cbd38fd0141b": {"node_ids": ["01031466-5c9c-423f-b7f3-fd8daea93440"], "metadata": {}}, "498af3cf-4320-4885-953d-7ba32db1c6b5": {"node_ids": ["e2fcdd68-a88e-493c-8bf9-80f4819c3871"], "metadata": {}}, "9e143ce2-864f-41be-96f1-5a0bf69de35f": {"node_ids": ["4e440a22-d429-400a-9629-61894ca41a74"], "metadata": {}}, "339615d0-f5d0-433d-9ac0-306877a93a7b": {"node_ids": ["991e4599-295d-4c87-a7dc-95bf18328d07"], "metadata": {}}, "07de0b85-5d5c-4e9e-bb0f-5229ae05e54f": {"node_ids": ["0e625d39-4fbd-484e-ba9d-d570fec90736"], "metadata": {}}, "2e4959f8-9e24-4bdd-a252-97dcdc6635b0": {"node_ids": ["f380dc30-8c8c-47ee-9dc5-6a9905a6686b"], "metadata": {}}, "8cb94f33-d107-4f9f-9011-72cea3304c74": {"node_ids": ["96d682e8-cb07-487c-a02f-37529fbf6dd7", "97237e60-5ade-426b-9f8b-05836002b1b3"], "metadata": {}}, "75a21967-0a6f-4ebe-bf81-3a71f06c60b5": {"node_ids": ["db2966a7-8354-40d1-9686-4132769d315c"], "metadata": {}}, "38f0359a-78c3-45d0-9c7a-197c1bf30cef": {"node_ids": ["0a179080-2643-4c5f-b8fa-0f5fb4a3fac6"], "metadata": {}}, "04d7ebc5-25bc-44b9-8989-8b22762e1bcb": {"node_ids": ["18b5855a-da3d-4584-824d-e19bcd3bba44"], "metadata": {}}, "dc24781d-5656-4316-9a7d-cc588e50d7bc": {"node_ids": ["a3374b56-7286-43c9-98f7-519553572b5b"], "metadata": {}}, "e374ceb0-c889-433f-ba89-d157a81e2013": {"node_ids": ["df9f576c-0cd5-41b2-a0d8-fab0e1950c79"], "metadata": {}}, "96cf3362-3210-436c-b929-54fb78fde30b": {"node_ids": ["47fdb869-eeb8-463b-ae30-688374ae0b26"], "metadata": {}}, "258e7ffd-9b5f-44c0-822c-e1e494ee93a0": {"node_ids": ["1ddf20be-db42-4c26-9b29-78da837c1715"], "metadata": {}}, "fbdeb71b-afda-4635-b4ab-fca1b6552ebf": {"node_ids": ["ccf64be1-25a9-4da9-b6d7-f694a3a3e988"], "metadata": {}}, "81e23f77-902f-41e0-bf0b-c1e86cb25b2c": {"node_ids": ["bdebe034-3c1c-48a8-a506-330c5fe931aa"], "metadata": {}}, "8dd55eee-e366-4973-901f-d974b47b8314": {"node_ids": ["4d7297a0-a687-4234-af3a-820f144a0af3"], "metadata": {}}, "12ae4899-a053-4ee3-8892-ab9c758e5361": {"node_ids": ["0a76e208-2432-4f0c-8391-01a8c9afb239", "97c7d7f6-5331-4672-9313-526ed8a7f5e6"], "metadata": {}}, "c390adb4-8ec2-4730-aef4-45823777b1df": {"node_ids": ["c99391ae-b035-410b-bc27-d8f44176a2ac", "df1823a8-f3da-4d8a-be8b-d1ccd0a5aef1"], "metadata": {}}, "e0d6fc45-5bf0-47b4-8aa8-458b8f9af142": {"node_ids": ["e1c69f8d-2abd-4eb3-a321-af818bf56394"], "metadata": {}}, "be3639b8-96ad-44fb-a5b9-d0e5326a546f": {"node_ids": ["6e788d27-0441-4418-8390-87bbf0ee3e47"], "metadata": {}}, "4de7f4fa-cf73-41cd-9ecd-96fb3e2a0bb2": {"node_ids": ["5dc1a04d-91f1-4904-96db-bbc4cc567810"], "metadata": {}}, "ecff2333-5b06-4ee3-883f-26b9a93db345": {"node_ids": ["754ce08f-2ebb-4ea8-aa43-9be63b847d82"], "metadata": {}}, "c9255197-4bd9-4166-a132-dfa29d9a3f72": {"node_ids": ["0d0e65c3-7a80-44b8-b0c2-edb997716aad", "6b024771-a7d8-487f-b6b4-97a86fde813d"], "metadata": {}}}}